{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:47:29.695838Z",
     "iopub.status.busy": "2024-11-15T12:47:29.695006Z",
     "iopub.status.idle": "2024-11-15T12:47:31.609416Z",
     "shell.execute_reply": "2024-11-15T12:47:31.608360Z",
     "shell.execute_reply.started": "2024-11-15T12:47:29.695798Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json\n",
    "\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# 把預訓練好的modelload進來\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# 设置设备（如果有GPU，则使用GPU）\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "def same_seeds(seed):\n",
    "\n",
    "\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "\n",
    "\n",
    "\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "same_seeds(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "\n",
    "\n",
    "- {train/test}set:\n",
    "\n",
    "\n",
    "\n",
    "  - List of dicts with the following keys:\n",
    "\n",
    "\n",
    "\n",
    "   - ID (string)\n",
    "\n",
    "\n",
    "\n",
    "   - Sentence (string)\n",
    "\n",
    "\n",
    "\n",
    "   - Aspect (string)\n",
    "\n",
    "\n",
    "\n",
    "   - AspectFromTo (string): start point#end point\n",
    "\n",
    "\n",
    "\n",
    "   - Category (string) : \"食物#品質\"\n",
    "\n",
    "\n",
    "\n",
    "   - Opinion (string)\n",
    "\n",
    "\n",
    "\n",
    "   - OpinionFromTo (string):  start point#end point\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   - Intensity (string) :  \"6.17#6.33\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- vaild set:\n",
    "\n",
    "\n",
    "\n",
    "  - List of[ID, Sentence] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:47:31.611660Z",
     "iopub.status.busy": "2024-11-15T12:47:31.611163Z",
     "iopub.status.idle": "2024-11-15T12:47:31.749015Z",
     "shell.execute_reply": "2024-11-15T12:47:31.747919Z",
     "shell.execute_reply.started": "2024-11-15T12:47:31.611625Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# 读取数据函数\n",
    "def read_data(file):\n",
    "    with open(file, 'r', encoding=\"utf-8\") as reader:\n",
    "        data = json.load(reader)\n",
    "    df = pd.json_normalize(data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 創建自定義的 Dataset 類別\n",
    "\n",
    "我們需要創建一個繼承自 torch.utils.data.Dataset 的類別，來處理資料並提供給 DataLoader。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:47:31.750520Z",
     "iopub.status.busy": "2024-11-15T12:47:31.750201Z",
     "iopub.status.idle": "2024-11-15T12:47:31.758246Z",
     "shell.execute_reply": "2024-11-15T12:47:31.757278Z",
     "shell.execute_reply.started": "2024-11-15T12:47:31.750478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "class TripletExtractionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels, intensity_labels):\n",
    "        self.input_ids = inputs['input_ids']\n",
    "        self.attention_mask = inputs['attention_mask']\n",
    "        self.labels = labels\n",
    "        self.intensity_labels = intensity_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            'intensity_labels': torch.tensor(self.intensity_labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Data\n",
    "\n",
    "\n",
    "\n",
    "将数据准备为 BERT 输入格式\n",
    "\n",
    "您可以提取 Sentence 字段并将其分词。为了准备 BERT 的输入，可以根据需要将 Aspect 和 Opinion 部分组合到句子中。这里假设要为每个 Sentence 生成分词结果并对齐其 Aspect 和 Opinion 信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:47:31.761425Z",
     "iopub.status.busy": "2024-11-15T12:47:31.761047Z",
     "iopub.status.idle": "2024-11-15T12:47:33.224464Z",
     "shell.execute_reply": "2024-11-15T12:47:33.223536Z",
     "shell.execute_reply.started": "2024-11-15T12:47:31.761383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 確認標籤數量\n",
    "unique_labels = set()\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "    for category in row['Category']:\n",
    "        unique_labels.add(category)\n",
    "\n",
    "# for i in unique_labels:\n",
    "\n",
    "    # print(i)\n",
    "\n",
    "num_labels = len(unique_labels)\n",
    "#print(\"num_labels:\", num_labels)\n",
    "\n",
    "# 使用BERT快速標記器來自動對齊標記和標籤\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese') # 使用支持 offset_mapping 的 BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:47:33.225899Z",
     "iopub.status.busy": "2024-11-15T12:47:33.225605Z",
     "iopub.status.idle": "2024-11-15T12:47:35.889236Z",
     "shell.execute_reply": "2024-11-15T12:47:35.888501Z",
     "shell.execute_reply.started": "2024-11-15T12:47:33.225866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 定义标签映射\n",
    "\n",
    "label_to_id = {'O': 0, 'B-Aspect': 1, 'I-Aspect': 2, 'B-Opinion': 3, 'I-Opinion': 4}\n",
    "\n",
    "\n",
    "\n",
    "def encode_labels(text, aspects, opinions, aspect_positions, opinion_positions, intensities):\n",
    "    # 使用相同的编码过程，确保 tokens 和 offset_mapping 对齐\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, truncation=True, max_length=32, add_special_tokens=True)\n",
    "    tokens = encoding.tokens()\n",
    "    offset_mapping = encoding['offset_mapping']\n",
    "    labels = ['O'] * len(tokens)\n",
    "    # 初始化 intensity_labels，每个 token 对应一个 [Valence, Arousal]，初始为 [5.0, 5.0]\n",
    "    intensity_labels = [[5.0, 5.0] for _ in range(len(tokens))]\n",
    "\n",
    "    # 创建一个 mapping，将每个字符位置映射到对应的 token 索引\n",
    "    char_to_token_map = {}\n",
    "    for idx, (start, end) in enumerate(offset_mapping):\n",
    "        if start == end:\n",
    "            continue  # 跳过特殊标记或填充部分\n",
    "        for char_pos in range(start, end):\n",
    "            char_to_token_map[char_pos] = idx\n",
    "\n",
    "    # 处理 Aspect 标签\n",
    "    for aspect, pos in zip(aspects, aspect_positions):\n",
    "        start_char, end_char = map(int, pos.split('#'))\n",
    "        start_char -= 1  # 调整为 0-based 索引\n",
    "        # 获取起始和结束 token 索引\n",
    "        try:\n",
    "            start_token_idx = char_to_token_map[start_char]\n",
    "            end_token_idx = char_to_token_map[end_char - 1]\n",
    "        except KeyError:\n",
    "            continue  # 如果字符位置不在映射中，跳过\n",
    "        labels[start_token_idx] = 'B-Aspect'\n",
    "        for idx in range(start_token_idx + 1, end_token_idx + 1):\n",
    "            labels[idx] = 'I-Aspect'\n",
    "\n",
    "    # 处理 Opinion 标签，并赋予对应的情感强度\n",
    "    for opinion, pos, intensity in zip(opinions, opinion_positions, intensities):\n",
    "        start_char, end_char = map(int, pos.split('#'))\n",
    "        start_char -= 1  # 调整为 0-based 索引\n",
    "        valence, arousal = map(float, intensity.split('#'))  # 解析情感强度值\n",
    "        try:\n",
    "            start_token_idx = char_to_token_map[start_char]\n",
    "            end_token_idx = char_to_token_map[end_char - 1]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        labels[start_token_idx] = 'B-Opinion'\n",
    "        intensity_labels[start_token_idx] = [valence, arousal]\n",
    "        for idx in range(start_token_idx + 1, end_token_idx + 1):\n",
    "            labels[idx] = 'I-Opinion'\n",
    "            intensity_labels[idx] = [valence, arousal]\n",
    "\n",
    "    # 将标签转换为ID\n",
    "    label_ids = [label_to_id[label] for label in labels]\n",
    "    return label_ids, intensity_labels\n",
    "\n",
    "\n",
    "\n",
    "def data_generator(df):\n",
    "    # 处理整个数据集\n",
    "    inputs = {'input_ids': [], 'attention_mask': []}\n",
    "    label_ids_list = []\n",
    "    intensity_labels_list = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['Sentence']\n",
    "        aspects = row['Aspect']\n",
    "        opinions = row['Opinion']\n",
    "        aspect_positions = row['AspectFromTo']\n",
    "        opinion_positions = row['OpinionFromTo']\n",
    "        intensities = row['Intensity']  # 获取情感强度\n",
    "\n",
    "        # 分词并编码\n",
    "        encoding = tokenizer(text, truncation=True, padding='max_length', max_length=32, return_offsets_mapping=True)\n",
    "\n",
    "        # 标签编码，获取 label_ids 和 intensity_labels\n",
    "        label_ids, intensity_labels = encode_labels(text, aspects, opinions, aspect_positions, opinion_positions, intensities)\n",
    "\n",
    "        # 填充或截断标签\n",
    "        seq_length = len(encoding['input_ids'])\n",
    "        if len(label_ids) < seq_length:\n",
    "            label_ids += [label_to_id['O']] * (seq_length - len(label_ids))\n",
    "            intensity_labels += [[0.0, 0.0]] * (seq_length - len(intensity_labels))\n",
    "        else:\n",
    "            label_ids = label_ids[:seq_length]\n",
    "            intensity_labels = intensity_labels[:seq_length]\n",
    "\n",
    "        # 移除 offset_mapping，因后续不需要\n",
    "        encoding.pop('offset_mapping')\n",
    "\n",
    "        # 添加到列表\n",
    "        inputs['input_ids'].append(encoding['input_ids'])\n",
    "        inputs['attention_mask'].append(encoding['attention_mask'])\n",
    "        label_ids_list.append(label_ids)\n",
    "        intensity_labels_list.append(intensity_labels)\n",
    "\n",
    "    # 创建 Dataset\n",
    "    dataset = TripletExtractionDataset(inputs, label_ids_list, intensity_labels_list)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ASTE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:51:02.143103Z",
     "iopub.status.busy": "2024-11-15T12:51:02.142692Z",
     "iopub.status.idle": "2024-11-15T12:51:02.156817Z",
     "shell.execute_reply": "2024-11-15T12:51:02.155900Z",
     "shell.execute_reply.started": "2024-11-15T12:51:02.143067Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimpleASTEModel(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(SimpleASTEModel, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)  # [batch_size, seq_len, hidden_size]\n",
    "        logits = self.classifier(sequence_output)  # [batch_size, seq_len, num_labels]\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            active_loss = attention_mask.view(-1) == 1  # 忽略填充部分\n",
    "            active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "            active_labels = labels.view(-1)[active_loss]\n",
    "            loss = loss_fct(active_logits, active_labels)\n",
    "        # print(f\"Sequence output shape: {sequence_output.shape}\")\n",
    "        # print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 辅助函数的定义\n",
    "1. 提取 BIO 标签对应的实体（方面和观点）:extract_spans\n",
    "2. 提取预测的三元组\n",
    "3. 提取真实的三元组\n",
    "4. 计算评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:56:12.749350Z",
     "iopub.status.busy": "2024-11-15T12:56:12.748916Z",
     "iopub.status.idle": "2024-11-15T12:56:12.769114Z",
     "shell.execute_reply": "2024-11-15T12:56:12.768081Z",
     "shell.execute_reply.started": "2024-11-15T12:56:12.749311Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_spans(tags, tokens, label_prefix):\n",
    "    spans = []\n",
    "    i = 0\n",
    "    while i < len(tags):\n",
    "        if tags[i] == f'B-{label_prefix}':\n",
    "            start = i\n",
    "            i += 1\n",
    "            while i < len(tags) and tags[i] == f'I-{label_prefix}':\n",
    "                i += 1\n",
    "            end = i  # 结束位置的下一个索引\n",
    "            span_tokens = tokens[start:end]\n",
    "            spans.append((start, end, span_tokens))\n",
    "        else:\n",
    "            i += 1\n",
    "    return spans\n",
    "\n",
    "def extract_triplets(tokens, tags, model, input_ids, attention_mask):\n",
    "    aspect_spans = extract_spans(tags, tokens, 'Aspect')\n",
    "    opinion_spans = extract_spans(tags, tokens, 'Opinion')\n",
    "\n",
    "    binary_aspect_opinion = []  # 1. Aspect-Opinion 二元组\n",
    "    v_triplets = []            # 2. 包含 Valence 的 triplets\n",
    "    a_triplets = []            # 3. 包含 Arousal 的 triplets\n",
    "    full_triplets = []         # 4. 原始的 Aspect-Opinion-Valence-Arousal 三元组\n",
    "\n",
    "    # 获取模型的序列输出\n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert(input_ids=input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))\n",
    "        sequence_output = outputs.last_hidden_state.squeeze(0)  # [seq_len, hidden_size]\n",
    "\n",
    "    for opinion_span in opinion_spans:\n",
    "        start, end, opinion_tokens = opinion_span\n",
    "\n",
    "        # 获取观点词的隐藏状态\n",
    "        opinion_hidden_states = sequence_output[start:end]\n",
    "\n",
    "        # 计算平均表示\n",
    "        opinion_representation = opinion_hidden_states.mean(dim=0)  # [hidden_size]\n",
    "\n",
    "        # 预测情感强度\n",
    "        intensity_pred = model.regressor(opinion_representation.unsqueeze(0))  # [1, 2]\n",
    "        intensity_pred = intensity_pred.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        # 原本只有round\n",
    "        valence = int(round(intensity_pred[0] * 10))\n",
    "        arousal = int(round(abs(intensity_pred[1] * 10)))\n",
    "\n",
    "        opinion_text = ''.join(opinion_tokens).replace('##', '')\n",
    "\n",
    "        if len(aspect_spans) == 0:\n",
    "            aspect_text = ''\n",
    "            binary_aspect_opinion.append((aspect_text, opinion_text))\n",
    "            v_triplets.append((aspect_text, opinion_text, valence))\n",
    "            a_triplets.append((aspect_text, opinion_text, arousal))\n",
    "            full_triplets.append((aspect_text, opinion_text, valence, arousal))\n",
    "        else:\n",
    "            for aspect_span in aspect_spans:\n",
    "                a_start, a_end, aspect_tokens = aspect_span\n",
    "                aspect_text = ''.join(aspect_tokens).replace('##', '')\n",
    "                binary_aspect_opinion.append((aspect_text, opinion_text))\n",
    "                v_triplets.append((aspect_text, opinion_text, valence))\n",
    "                a_triplets.append((aspect_text, opinion_text, arousal))\n",
    "                full_triplets.append((aspect_text, opinion_text, valence, arousal))\n",
    "\n",
    "    return binary_aspect_opinion, v_triplets, a_triplets, full_triplets\n",
    "\n",
    "\n",
    "def extract_gold_triplets(tokens, tags, intensity_labels):\n",
    "    aspect_spans = extract_spans(tags, tokens, 'Aspect')\n",
    "    opinion_spans = extract_spans(tags, tokens, 'Opinion')\n",
    "\n",
    "    binary_aspect_opinion = []  # 1. Aspect-Opinion 二元组\n",
    "    v_triplets = []            # 2. 包含 Valence 的 triplets\n",
    "    a_triplets = []            # 3. 包含 Arousal 的 triplets\n",
    "    full_triplets = []         # 4. 原始的 Aspect-Opinion-Valence-Arousal 三元组\n",
    "\n",
    "    for opinion_span in opinion_spans:\n",
    "        start, end, opinion_tokens = opinion_span\n",
    "\n",
    "        # 获取观点词的情感强度标签\n",
    "        intensity_values = intensity_labels[start:end]  # [span_len, 2]\n",
    "        intensity_values = intensity_values.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "        valence = int(round(intensity_values[0]))\n",
    "        arousal = int(round(intensity_values[1]))\n",
    "\n",
    "        opinion_text = ''.join(opinion_tokens).replace('##', '')\n",
    "\n",
    "        if len(aspect_spans) == 0:\n",
    "            aspect_text = ''\n",
    "            binary_aspect_opinion.append((aspect_text, opinion_text))\n",
    "            v_triplets.append((aspect_text, opinion_text, valence))\n",
    "            a_triplets.append((aspect_text, opinion_text, arousal))\n",
    "            full_triplets.append((aspect_text, opinion_text, valence, arousal))\n",
    "        else:\n",
    "            for aspect_span in aspect_spans:\n",
    "                a_start, a_end, aspect_tokens = aspect_span\n",
    "                aspect_text = ''.join(aspect_tokens).replace('##', '')\n",
    "                binary_aspect_opinion.append((aspect_text, opinion_text))\n",
    "                v_triplets.append((aspect_text, opinion_text, valence))\n",
    "                a_triplets.append((aspect_text, opinion_text, arousal))\n",
    "                full_triplets.append((aspect_text, opinion_text, valence, arousal))\n",
    "\n",
    "    return binary_aspect_opinion, v_triplets, a_triplets, full_triplets\n",
    "\n",
    "\n",
    "def compute_metrics(pred_triplets, gold_triplets):\n",
    "\n",
    "    \n",
    "    pred_set = set(pred_triplets)\n",
    "    gold_set = set(gold_triplets)\n",
    "\n",
    "    true_positives = pred_set & gold_set\n",
    "    # if(true_positives):\n",
    "    #     print(pred_triplets[:3])\n",
    "    #     print(gold_triplets[:3])\n",
    "    # print(true_positives)\n",
    "        \n",
    "    precision = len(true_positives) / len(pred_set) if len(pred_set) > 0 else 0.0\n",
    "    recall = len(true_positives) / len(gold_set) if len(gold_set) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:45:20.314433Z",
     "iopub.status.busy": "2024-11-15T13:45:20.313560Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SimpleASTEModel were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\abc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.30141684094890514\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O     0.9421    0.9233    0.9326      4863\n",
      "    B-Aspect     0.9253    0.9114    0.9183       734\n",
      "    I-Aspect     0.9166    0.9239    0.9202      1498\n",
      "   B-Opinion     0.8834    0.8855    0.8844       847\n",
      "   I-Opinion     0.8834    0.9313    0.9067      1733\n",
      "\n",
      "    accuracy                         0.9206      9675\n",
      "   macro avg     0.9102    0.9151    0.9125      9675\n",
      "weighted avg     0.9212    0.9206    0.9208      9675\n",
      "\n",
      "Epoch 2, Loss: 0.14351983266005738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O     0.9586    0.9134    0.9355      4863\n",
      "    B-Aspect     0.9179    0.9292    0.9235       734\n",
      "    I-Aspect     0.9029    0.9559    0.9287      1498\n",
      "   B-Opinion     0.8834    0.9032    0.8932       847\n",
      "   I-Opinion     0.8808    0.9383    0.9086      1733\n",
      "\n",
      "    accuracy                         0.9248      9675\n",
      "   macro avg     0.9087    0.9280    0.9179      9675\n",
      "weighted avg     0.9264    0.9248    0.9250      9675\n",
      "\n",
      "Epoch 3, Loss: 0.08713298033901125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O     0.9547    0.9266    0.9404      4863\n",
      "    B-Aspect     0.9293    0.9305    0.9299       734\n",
      "    I-Aspect     0.9104    0.9566    0.9329      1498\n",
      "   B-Opinion     0.8866    0.9044    0.8954       847\n",
      "   I-Opinion     0.9001    0.9256    0.9127      1733\n",
      "\n",
      "    accuracy                         0.9294      9675\n",
      "   macro avg     0.9162    0.9287    0.9223      9675\n",
      "weighted avg     0.9302    0.9294    0.9295      9675\n",
      "\n",
      "Model saved at E:\\NYCU-Project\\Class\\NLP\\Dimensional ASTE\\log\\trained_model.pt\n",
      "Tokenizer saved at E:\\NYCU-Project\\Class\\NLP\\Dimensional ASTE\\log\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the save path\n",
    "save_path = r\"E:\\NYCU-Project\\Class\\NLP\\Dimensional ASTE\\log\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# 加載數據集並設置標籤\n",
    "data_path = r'E:\\NYCU-Project\\Class\\NLP\\Dimensional ASTE\\NYCU_NLP_113A_Dataset\\NYCU_NLP_113A_TrainingSet.json'\n",
    "df = read_data(data_path)\n",
    "\n",
    "label_to_id = {'O': 0, 'B-Aspect': 1, 'I-Aspect': 2, 'B-Opinion': 3, 'I-Opinion': 4}\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "num_labels = len(label_to_id)\n",
    "\n",
    "# 分割訓練和驗證集\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = data_generator(train_df)\n",
    "val_dataset = data_generator(val_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=32)\n",
    "val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=32)\n",
    "\n",
    "# 加載模型\n",
    "config = BertConfig.from_pretrained('bert-base-chinese', num_labels=num_labels)\n",
    "model = SimpleASTEModel.from_pretrained('bert-base-chinese', config=config)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "num_training_steps = len(train_loader) * 3  # 假設訓練 3 個 epoch\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# 使用BERT快速標記器來自動對齊標記和標籤\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese') # 使用支持 offset_mapping 的 BertTokenizerFast\n",
    "# 訓練\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch_input_ids = batch['input_ids'].to(device)\n",
    "        batch_attention_mask = batch['attention_mask'].to(device)\n",
    "        batch_labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_attention_masks = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch_input_ids = batch['input_ids'].to(device)\n",
    "            batch_attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=2)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_attention_masks.extend(batch_attention_mask.cpu().numpy())\n",
    "\n",
    "    # 展平預測和標籤\n",
    "    flat_preds = []\n",
    "    flat_labels = []\n",
    "\n",
    "    for pred, label, mask in zip(all_preds, all_labels, all_attention_masks):\n",
    "        active_indices = mask.flatten() == 1\n",
    "        flat_preds.extend(pred.flatten()[active_indices])\n",
    "        flat_labels.extend(label.flatten()[active_indices])\n",
    "\n",
    "    # 將數值標籤轉換為標籤名稱\n",
    "    flat_preds = [id_to_label[pred_id] for pred_id in flat_preds]\n",
    "    flat_labels = [id_to_label[label_id] for label_id in flat_labels]\n",
    "    # print(flat_labels[:3])\n",
    "\n",
    "    # 計算分類報告\n",
    "    report = classification_report(flat_labels, flat_preds, labels=list(label_to_id.keys()), digits=4)\n",
    "\n",
    "    print(report)\n",
    "    # After training is complete\n",
    "\n",
    "# Save the model and tokenizer after training\n",
    "model_save_path = os.path.join(save_path, \"trained_model.pt\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved at {model_save_path}\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"Tokenizer saved at {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:33:38.989099Z",
     "iopub.status.busy": "2024-11-15T13:33:38.988417Z",
     "iopub.status.idle": "2024-11-15T13:33:39.270020Z",
     "shell.execute_reply": "2024-11-15T13:33:39.269057Z",
     "shell.execute_reply.started": "2024-11-15T13:33:38.989060Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from E:\\NYCU-Project\\Class\\NLP\\Dimensional ASTE\\log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SimpleASTEModel were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\abc\\AppData\\Local\\Temp\\ipykernel_14260\\3806278991.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_load_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['上', '菜', '速', '度', '相', '當', '的', '快', '，', '服', '務', '人', '員', '也', '很', '親', '切']\n",
      "Predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Predicted Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Input Sentence: 上菜速度相當的快，服務人員也很親切\n",
      "Aspects: []\n",
      "Opinions: []\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertConfig\n",
    "\n",
    "# Load the tokenizer from the saved path\n",
    "tokenizer = BertTokenizerFast.from_pretrained(save_path)\n",
    "print(f\"Tokenizer loaded from {save_path}\")\n",
    "\n",
    "# Label mappings\n",
    "label_to_id = {'O': 0, 'B-Aspect': 1, 'I-Aspect': 2, 'B-Opinion': 3, 'I-Opinion': 4}\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "num_labels = len(label_to_id)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model configuration\n",
    "config = BertConfig.from_pretrained('bert-base-chinese', num_labels=num_labels)\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleASTEModel.from_pretrained('bert-base-chinese', config=config)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model_load_path = os.path.join(save_path, \"trained_model.pt\")\n",
    "model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    encoding = tokenizer(sentence, return_offsets_mapping=True, add_special_tokens=False)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    return tokens, input_ids, attention_mask\n",
    "\n",
    "def create_input_tensors(input_ids, attention_mask):\n",
    "    input_ids_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    attention_mask_tensor = torch.tensor([attention_mask], dtype=torch.long).to(device)\n",
    "    return input_ids_tensor, attention_mask_tensor\n",
    "\n",
    "def get_predictions(model, input_ids_tensor, attention_mask_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids_tensor, attention_mask=attention_mask_tensor)\n",
    "        predictions = torch.argmax(outputs, dim=2)  # outputs is logits tensor\n",
    "        predictions = predictions.cpu().numpy()[0]\n",
    "    return predictions\n",
    "\n",
    "def map_predictions_to_labels(predictions):\n",
    "    predicted_labels = [id_to_label.get(pred_id, 'O') for pred_id in predictions]\n",
    "    return predicted_labels\n",
    "\n",
    "def extract_terms(tokens, predicted_labels):\n",
    "    aspects = []\n",
    "    opinions = []\n",
    "    i = 0\n",
    "    while i < len(predicted_labels):\n",
    "        label = predicted_labels[i]\n",
    "        if label == 'B-Aspect':\n",
    "            aspect_tokens = [tokens[i]]\n",
    "            i += 1\n",
    "            while i < len(predicted_labels) and predicted_labels[i] == 'I-Aspect':\n",
    "                aspect_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "            aspect = ''.join(aspect_tokens).replace('##', '')\n",
    "            aspects.append(aspect)\n",
    "        elif label == 'B-Opinion':\n",
    "            opinion_tokens = [tokens[i]]\n",
    "            i += 1\n",
    "            while i < len(predicted_labels) and predicted_labels[i] == 'I-Opinion':\n",
    "                opinion_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "            opinion = ''.join(opinion_tokens).replace('##', '')\n",
    "            opinions.append(opinion)\n",
    "        else:\n",
    "            i += 1\n",
    "    return aspects, opinions\n",
    "\n",
    "def test_model(sentence):\n",
    "    tokens, input_ids, attention_mask = preprocess_sentence(sentence)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    input_ids_tensor, attention_mask_tensor = create_input_tensors(input_ids, attention_mask)\n",
    "    predictions = get_predictions(model, input_ids_tensor, attention_mask_tensor)\n",
    "    print(\"Predictions:\", predictions)\n",
    "    predicted_labels = map_predictions_to_labels(predictions)\n",
    "    print(\"Predicted Labels:\", predicted_labels)\n",
    "    aspects, opinions = extract_terms(tokens, predicted_labels)\n",
    "    print(\"Input Sentence:\", sentence)\n",
    "    print(\"Aspects:\", aspects)\n",
    "    print(\"Opinions:\", opinions)\n",
    "\n",
    "# Test the model\n",
    "test_sentence = \"上菜速度相當的快，服務人員也很親切\"\n",
    "test_model(test_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6076231,
     "sourceId": 9893197,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
