{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:47:29.695838Z",
     "iopub.status.busy": "2024-11-15T12:47:29.695006Z",
     "iopub.status.idle": "2024-11-15T12:47:31.609416Z",
     "shell.execute_reply": "2024-11-15T12:47:31.608360Z",
     "shell.execute_reply.started": "2024-11-15T12:47:29.695798Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json\n",
    "\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# 把預訓練好的modelload進來\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# 设置设备（如果有GPU，则使用GPU）\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "def same_seeds(seed):\n",
    "\n",
    "\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "\n",
    "\n",
    "\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "same_seeds(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "\n",
    "\n",
    "- {train/test}set:\n",
    "\n",
    "\n",
    "\n",
    "  - List of dicts with the following keys:\n",
    "\n",
    "\n",
    "\n",
    "   - ID (string)\n",
    "\n",
    "\n",
    "\n",
    "   - Sentence (string)\n",
    "\n",
    "\n",
    "\n",
    "   - Aspect (string)\n",
    "\n",
    "\n",
    "\n",
    "   - AspectFromTo (string): start point#end point\n",
    "\n",
    "\n",
    "\n",
    "   - Category (string) : \"食物#品質\"\n",
    "\n",
    "\n",
    "\n",
    "   - Opinion (string)\n",
    "\n",
    "\n",
    "\n",
    "   - OpinionFromTo (string):  start point#end point\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   - Intensity (string) :  \"6.17#6.33\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- vaild set:\n",
    "\n",
    "\n",
    "\n",
    "  - List of[ID, Sentence] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:47:31.611660Z",
     "iopub.status.busy": "2024-11-15T12:47:31.611163Z",
     "iopub.status.idle": "2024-11-15T12:47:31.749015Z",
     "shell.execute_reply": "2024-11-15T12:47:31.747919Z",
     "shell.execute_reply.started": "2024-11-15T12:47:31.611625Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "\n",
    "    with open(file, 'r', encoding=\"utf-8\") as reader:\n",
    "\n",
    "        data = json.load(reader)\n",
    "    # 將JSON數據加載到Pandas DataFrame中\n",
    "    df = pd.json_normalize(data)\n",
    "    # 将数据分成训练集和测试集\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    return train_df, test_df\n",
    "# 读取训练集和开发集\n",
    "\n",
    "train_df, test_df = read_data(r'E:\\NYCU-Project\\Class\\NLP\\Dimensional ASTE\\NYCU_NLP_113A_Dataset\\NYCU_NLP_113A_TrainingSet.json')\n",
    "\n",
    "# test_questions, test_paragraphs = read_data('/kaggle/input/ml2021-spring-hw7/hw7_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 創建自定義的 Dataset 類別\n",
    "\n",
    "我們需要創建一個繼承自 torch.utils.data.Dataset 的類別，來處理資料並提供給 DataLoader。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:47:31.750520Z",
     "iopub.status.busy": "2024-11-15T12:47:31.750201Z",
     "iopub.status.idle": "2024-11-15T12:47:31.758246Z",
     "shell.execute_reply": "2024-11-15T12:47:31.757278Z",
     "shell.execute_reply.started": "2024-11-15T12:47:31.750478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "class TripletExtractionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels, intensity_labels):\n",
    "        self.input_ids = inputs['input_ids']\n",
    "        self.attention_mask = inputs['attention_mask']\n",
    "        self.labels = labels\n",
    "        self.intensity_labels = intensity_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            'intensity_labels': torch.tensor(self.intensity_labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Data\n",
    "\n",
    "\n",
    "\n",
    "将数据准备为 BERT 输入格式\n",
    "\n",
    "您可以提取 Sentence 字段并将其分词。为了准备 BERT 的输入，可以根据需要将 Aspect 和 Opinion 部分组合到句子中。这里假设要为每个 Sentence 生成分词结果并对齐其 Aspect 和 Opinion 信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:47:31.761425Z",
     "iopub.status.busy": "2024-11-15T12:47:31.761047Z",
     "iopub.status.idle": "2024-11-15T12:47:33.224464Z",
     "shell.execute_reply": "2024-11-15T12:47:33.223536Z",
     "shell.execute_reply.started": "2024-11-15T12:47:31.761383Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abc\\.cache\\huggingface\\hub\\models--bert-base-chinese. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# 確認標籤數量\n",
    "\n",
    "unique_labels = set()\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "\n",
    "    for category in row['Category']:\n",
    "\n",
    "        unique_labels.add(category)\n",
    "\n",
    "\n",
    "\n",
    "# for i in unique_labels:\n",
    "\n",
    "    # print(i)\n",
    "\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "#print(\"num_labels:\", num_labels)\n",
    "\n",
    "model_name = 'bert-base-chinese'\n",
    "\n",
    "# 使用BERT快速標記器來自動對齊標記和標籤\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese') # 使用支持 offset_mapping 的 BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:47:33.225899Z",
     "iopub.status.busy": "2024-11-15T12:47:33.225605Z",
     "iopub.status.idle": "2024-11-15T12:47:35.889236Z",
     "shell.execute_reply": "2024-11-15T12:47:35.888501Z",
     "shell.execute_reply.started": "2024-11-15T12:47:33.225866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 定义标签映射\n",
    "\n",
    "label_to_id = {'O': 0, 'B-Aspect': 1, 'I-Aspect': 2, 'B-Opinion': 3, 'I-Opinion': 4}\n",
    "\n",
    "\n",
    "\n",
    "def encode_labels(text, aspects, opinions, aspect_positions, opinion_positions, intensities):\n",
    "    # 使用相同的编码过程，确保 tokens 和 offset_mapping 对齐\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, truncation=True, max_length=32, add_special_tokens=True)\n",
    "    tokens = encoding.tokens()\n",
    "    offset_mapping = encoding['offset_mapping']\n",
    "    labels = ['O'] * len(tokens)\n",
    "    # 初始化 intensity_labels，每个 token 对应一个 [Valence, Arousal]，初始为 [0.0, 0.0]\n",
    "    intensity_labels = [[0.0, 0.0] for _ in range(len(tokens))]\n",
    "\n",
    "    # 创建一个 mapping，将每个字符位置映射到对应的 token 索引\n",
    "    char_to_token_map = {}\n",
    "    for idx, (start, end) in enumerate(offset_mapping):\n",
    "        if start == end:\n",
    "            continue  # 跳过特殊标记或填充部分\n",
    "        for char_pos in range(start, end):\n",
    "            char_to_token_map[char_pos] = idx\n",
    "\n",
    "    # 处理 Aspect 标签\n",
    "    for aspect, pos in zip(aspects, aspect_positions):\n",
    "        start_char, end_char = map(int, pos.split('#'))\n",
    "        start_char -= 1  # 调整为 0-based 索引\n",
    "        # 获取起始和结束 token 索引\n",
    "        try:\n",
    "            start_token_idx = char_to_token_map[start_char]\n",
    "            end_token_idx = char_to_token_map[end_char - 1]\n",
    "        except KeyError:\n",
    "            continue  # 如果字符位置不在映射中，跳过\n",
    "        labels[start_token_idx] = 'B-Aspect'\n",
    "        for idx in range(start_token_idx + 1, end_token_idx + 1):\n",
    "            labels[idx] = 'I-Aspect'\n",
    "\n",
    "    # 处理 Opinion 标签，并赋予对应的情感强度\n",
    "    for opinion, pos, intensity in zip(opinions, opinion_positions, intensities):\n",
    "        start_char, end_char = map(int, pos.split('#'))\n",
    "        start_char -= 1  # 调整为 0-based 索引\n",
    "        valence, arousal = map(float, intensity.split('#'))  # 解析情感强度值\n",
    "        try:\n",
    "            start_token_idx = char_to_token_map[start_char]\n",
    "            end_token_idx = char_to_token_map[end_char - 1]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        labels[start_token_idx] = 'B-Opinion'\n",
    "        intensity_labels[start_token_idx] = [valence, arousal]\n",
    "        for idx in range(start_token_idx + 1, end_token_idx + 1):\n",
    "            labels[idx] = 'I-Opinion'\n",
    "            intensity_labels[idx] = [valence, arousal]\n",
    "\n",
    "    # 将标签转换为ID\n",
    "    label_ids = [label_to_id[label] for label in labels]\n",
    "    return label_ids, intensity_labels\n",
    "\n",
    "\n",
    "\n",
    "def data_generator(df):\n",
    "    # 处理整个数据集\n",
    "    inputs = {'input_ids': [], 'attention_mask': []}\n",
    "    label_ids_list = []\n",
    "    intensity_labels_list = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['Sentence']\n",
    "        aspects = row['Aspect']\n",
    "        opinions = row['Opinion']\n",
    "        aspect_positions = row['AspectFromTo']\n",
    "        opinion_positions = row['OpinionFromTo']\n",
    "        intensities = row['Intensity']  # 获取情感强度\n",
    "\n",
    "        # 分词并编码\n",
    "        encoding = tokenizer(text, truncation=True, padding='max_length', max_length=32, return_offsets_mapping=True)\n",
    "\n",
    "        # 标签编码，获取 label_ids 和 intensity_labels\n",
    "        label_ids, intensity_labels = encode_labels(text, aspects, opinions, aspect_positions, opinion_positions, intensities)\n",
    "\n",
    "        # 填充或截断标签\n",
    "        seq_length = len(encoding['input_ids'])\n",
    "        if len(label_ids) < seq_length:\n",
    "            label_ids += [label_to_id['O']] * (seq_length - len(label_ids))\n",
    "            intensity_labels += [[0.0, 0.0]] * (seq_length - len(intensity_labels))\n",
    "        else:\n",
    "            label_ids = label_ids[:seq_length]\n",
    "            intensity_labels = intensity_labels[:seq_length]\n",
    "\n",
    "        # 移除 offset_mapping，因后续不需要\n",
    "        encoding.pop('offset_mapping')\n",
    "\n",
    "        # 添加到列表\n",
    "        inputs['input_ids'].append(encoding['input_ids'])\n",
    "        inputs['attention_mask'].append(encoding['attention_mask'])\n",
    "        label_ids_list.append(label_ids)\n",
    "        intensity_labels_list.append(intensity_labels)\n",
    "\n",
    "    # 创建 Dataset\n",
    "    dataset = TripletExtractionDataset(inputs, label_ids_list, intensity_labels_list)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "Train_dataset = data_generator(train_df)\n",
    "test_dataset = data_generator(test_df)\n",
    "\n",
    "# 创建DataLoader\n",
    "\n",
    "train_loader = DataLoader(Train_dataset, sampler=RandomSampler(Train_dataset), batch_size=8)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:47:35.890573Z",
     "iopub.status.busy": "2024-11-15T12:47:35.890292Z",
     "iopub.status.idle": "2024-11-15T12:47:35.897823Z",
     "shell.execute_reply": "2024-11-15T12:47:35.896879Z",
     "shell.execute_reply.started": "2024-11-15T12:47:35.890543Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # 定义模型\n",
    "# model = BertForTokenClassification.from_pretrained('bert-base-chinese', num_labels=len(label_to_id))\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "# # 训练参数\n",
    "# epochs = 3\n",
    "# learning_rate = 5e-5\n",
    "# validation = True\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "\n",
    "# total_steps = len(train_loader) * epochs\n",
    "\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# print(\"开始训练...\")\n",
    "\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "\n",
    "#     print(f'正在训练第 {epoch + 1}/{epochs} 个 epoch')\n",
    "\n",
    "#     model.train()\n",
    "\n",
    "#     total_loss = 0  # 初始化本 epoch 的总损失\n",
    "\n",
    "#     # 训练步骤\n",
    "\n",
    "#     for step, batch in enumerate(tqdm(train_loader)):\n",
    "\n",
    "#         # print(f\"Batch type: {type(batch)}\")\n",
    "\n",
    "#         # print(f\"Batch keys: {batch.keys() if isinstance(batch, dict) else 'Not a dict'}\")\n",
    "#         batch_input_ids = batch['input_ids'].to(device)\n",
    "#         batch_attention_masks = batch['attention_mask'].to(device)\n",
    "#         batch_labels = batch['labels'].to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n",
    "#         loss = outputs.loss\n",
    "\n",
    "\n",
    "\n",
    "#         loss.backward()\n",
    "\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "#         scheduler.step()\n",
    "#         total_loss += loss.item()\n",
    "        \n",
    "#     avg_train_loss = total_loss / len(train_loader)\n",
    "#     print(f'平均训练损失: {avg_train_loss}')\n",
    "\n",
    "#     if validation:\n",
    "#         model.eval()\n",
    "#         eval_loss = 0\n",
    "#         eval_accuracy = 0\n",
    "#         nb_eval_steps = 0\n",
    "\n",
    "#         for batch in test_loader:\n",
    "#             batch_input_ids = batch['input_ids'].to(device)\n",
    "#             batch_attention_masks = batch['attention_mask'].to(device)\n",
    "#             batch_labels = batch['labels'].to(device)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "\n",
    "#                 outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n",
    "\n",
    "#             loss = outputs.loss\n",
    "#             logits = outputs.logits\n",
    "#             eval_loss += loss.item()\n",
    "\n",
    "#             predictions = torch.argmax(logits, dim=2).detach().cpu().numpy()\n",
    "#             labels = batch_labels.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "#             # 忽略填充标签 (-100) 计算准确度\n",
    "\n",
    "#             for i in range(len(labels)):\n",
    "\n",
    "#                 valid_labels = labels[i] != -100\n",
    "\n",
    "#                 eval_accuracy += np.sum(predictions[i][valid_labels] == labels[i][valid_labels]) / np.sum(valid_labels)\n",
    "\n",
    "#                 nb_eval_steps += 1\n",
    "\n",
    "#         avg_eval_loss = eval_loss / len(test_loader)\n",
    "#         avg_eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "#         print(f'验证损失: {avg_eval_loss}')\n",
    "#         print(f'验证准确度: {avg_eval_accuracy}')\n",
    "       \n",
    "# # 保存模型和分词器\n",
    "# model.save_pretrained('saved_model')\n",
    "# tokenizer.save_pretrained('saved_model')\n",
    "# print(\"训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ASTE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:51:02.143103Z",
     "iopub.status.busy": "2024-11-15T12:51:02.142692Z",
     "iopub.status.idle": "2024-11-15T12:51:02.156817Z",
     "shell.execute_reply": "2024-11-15T12:51:02.155900Z",
     "shell.execute_reply.started": "2024-11-15T12:51:02.143067Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "\n",
    "class ASTEModel(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(ASTEModel, self).__init__(config)\n",
    "        self.num_labels = config.num_labels  # 序列标注的标签数量\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # 序列标注层\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        # 情感强度回归层\n",
    "        self.regressor = nn.Linear(config.hidden_size, 2)  # 输出 Valence 和 Arousal\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None, intensity_labels=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)  # [batch_size, seq_len, hidden_size]\n",
    "    \n",
    "        # 序列标注的 logits\n",
    "        logits = self.classifier(sequence_output)  # [batch_size, seq_len, num_labels]\n",
    "    \n",
    "        total_loss = None\n",
    "    \n",
    "        # 计算序列标注的损失\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            active_loss = attention_mask.view(-1) == 1  # 只计算非填充部分的损失\n",
    "            active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "            active_labels = labels.view(-1)[active_loss]\n",
    "            loss = loss_fct(active_logits, active_labels)\n",
    "            total_loss = loss\n",
    "    \n",
    "        # 计算情感强度的损失\n",
    "        if intensity_labels is not None:\n",
    "            # 获取观点词的位置\n",
    "            opinion_mask = ((labels == 3) | (labels == 4)) & attention_mask  # [batch_size, seq_len]\n",
    "            # 将张量重塑为 [batch_size * seq_len, ...]\n",
    "            batch_size, seq_len, hidden_size = sequence_output.size()\n",
    "            sequence_output_flat = sequence_output.view(-1, hidden_size)  # [batch_size * seq_len, hidden_size]\n",
    "            opinion_mask_flat = opinion_mask.view(-1)  # [batch_size * seq_len]\n",
    "            # 提取观点词的隐藏状态\n",
    "            opinion_outputs = sequence_output_flat[opinion_mask_flat]  # [num_opinions, hidden_size]\n",
    "            if opinion_outputs.size(0) > 0:\n",
    "                # 预测情感强度\n",
    "                intensity_preds = self.regressor(opinion_outputs)  # [num_opinions, 2]\n",
    "                # 取对应的情感强度标签\n",
    "                intensity_labels_flat = intensity_labels.view(-1, 2)  # [batch_size * seq_len, 2]\n",
    "                intensity_targets = intensity_labels_flat[opinion_mask_flat]  # [num_opinions, 2]\n",
    "                # 计算回归损失\n",
    "                loss_mse = nn.MSELoss()\n",
    "                regression_loss = loss_mse(intensity_preds, intensity_targets)\n",
    "                total_loss = total_loss + regression_loss if total_loss is not None else regression_loss\n",
    "    \n",
    "        output = (logits,)\n",
    "        return ((total_loss,) + output) if total_loss is not None else output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 辅助函数的定义\n",
    "1. 提取 BIO 标签对应的实体（方面和观点）:extract_spans\n",
    "2. 提取预测的三元组\n",
    "3. 提取真实的三元组\n",
    "4. 计算评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:56:12.749350Z",
     "iopub.status.busy": "2024-11-15T12:56:12.748916Z",
     "iopub.status.idle": "2024-11-15T12:56:12.769114Z",
     "shell.execute_reply": "2024-11-15T12:56:12.768081Z",
     "shell.execute_reply.started": "2024-11-15T12:56:12.749311Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_spans(tags, tokens, label_prefix):\n",
    "    spans = []\n",
    "    i = 0\n",
    "    while i < len(tags):\n",
    "        if tags[i] == f'B-{label_prefix}':\n",
    "            start = i\n",
    "            i += 1\n",
    "            while i < len(tags) and tags[i] == f'I-{label_prefix}':\n",
    "                i += 1\n",
    "            end = i  # 结束位置的下一个索引\n",
    "            span_tokens = tokens[start:end]\n",
    "            spans.append((start, end, span_tokens))\n",
    "        else:\n",
    "            i += 1\n",
    "    return spans\n",
    "\n",
    "\n",
    "def extract_triplets(tokens, tags, model, input_ids, attention_mask):\n",
    "    aspect_spans = extract_spans(tags, tokens, 'Aspect')\n",
    "    opinion_spans = extract_spans(tags, tokens, 'Opinion')\n",
    "\n",
    "    triplets = []\n",
    "\n",
    "    # 获取模型的序列输出\n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert(input_ids=input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))\n",
    "        sequence_output = outputs.last_hidden_state.squeeze(0)  # [seq_len, hidden_size]\n",
    "\n",
    "    for opinion_span in opinion_spans:\n",
    "        start, end, opinion_tokens = opinion_span\n",
    "\n",
    "        # 获取观点词的隐藏状态\n",
    "        opinion_hidden_states = sequence_output[start:end]\n",
    "\n",
    "        # 计算平均表示\n",
    "        opinion_representation = opinion_hidden_states.mean(dim=0)  # [hidden_size]\n",
    "\n",
    "        # 预测情感强度\n",
    "        intensity_pred = model.regressor(opinion_representation.unsqueeze(0))  # [1, 2]\n",
    "        intensity_pred = intensity_pred.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        # 将 Valence 和 Arousal 四舍五入为整数\n",
    "        valence = int(round(intensity_pred[0]))\n",
    "        arousal = int(round(intensity_pred[1]))\n",
    "\n",
    "        # 获取观点词文本\n",
    "        opinion_text = ''.join(opinion_tokens).replace('##', '')\n",
    "\n",
    "        # 处理对应的方面词\n",
    "        if len(aspect_spans) == 0:\n",
    "            # 如果没有检测到方面词\n",
    "            aspect_text = ''\n",
    "            triplet = (aspect_text, opinion_text, f\"{valence}#{arousal}\")\n",
    "            triplets.append(triplet)\n",
    "        else:\n",
    "            # 如果有多个方面词，可以根据需要进行匹配\n",
    "            for aspect_span in aspect_spans:\n",
    "                a_start, a_end, aspect_tokens = aspect_span\n",
    "                aspect_text = ''.join(aspect_tokens).replace('##', '')\n",
    "                triplet = (aspect_text, opinion_text, f\"{valence}#{arousal}\")\n",
    "                triplets.append(triplet)\n",
    "\n",
    "    return triplets\n",
    "\n",
    "def extract_gold_triplets(tokens, tags, intensity_labels):\n",
    "    aspect_spans = extract_spans(tags, tokens, 'Aspect')\n",
    "    opinion_spans = extract_spans(tags, tokens, 'Opinion')\n",
    "\n",
    "    triplets = []\n",
    "\n",
    "    for opinion_span in opinion_spans:\n",
    "        start, end, opinion_tokens = opinion_span\n",
    "\n",
    "        # 获取观点词的情感强度标签\n",
    "        intensity_values = intensity_labels[start:end]  # [span_len, 2]\n",
    "        intensity_values = intensity_values.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "        # 将 Valence 和 Arousal 四舍五入为整数\n",
    "        valence = int(round(intensity_values[0]))\n",
    "        arousal = int(round(intensity_values[1]))\n",
    "\n",
    "        opinion_text = ''.join(opinion_tokens).replace('##', '')\n",
    "\n",
    "        if len(aspect_spans) == 0:\n",
    "            aspect_text = ''\n",
    "            triplet = (aspect_text, opinion_text, f\"{valence}#{arousal}\")\n",
    "            triplets.append(triplet)\n",
    "        else:\n",
    "            for aspect_span in aspect_spans:\n",
    "                a_start, a_end, aspect_tokens = aspect_span\n",
    "                aspect_text = ''.join(aspect_tokens).replace('##', '')\n",
    "                triplet = (aspect_text, opinion_text, f\"{valence}#{arousal}\")\n",
    "                triplets.append(triplet)\n",
    "\n",
    "    return triplets\n",
    "\n",
    "\n",
    "def compute_metrics(pred_triplets, gold_triplets):\n",
    "    pred_set = set(pred_triplets)\n",
    "    gold_set = set(gold_triplets)\n",
    "\n",
    "    true_positives = pred_set & gold_set\n",
    "    precision = len(true_positives) / len(pred_set) if len(pred_set) > 0 else 0.0\n",
    "    recall = len(true_positives) / len(gold_set) if len(gold_set) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:45:20.314433Z",
     "iopub.status.busy": "2024-11-15T13:45:20.313560Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTEModel were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight', 'regressor.bias', 'regressor.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\abc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/605 [00:00<?, ?it/s]c:\\Users\\abc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "  2%|▏         | 12/605 [00:01<01:12,  8.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 50\u001b[0m\n\u001b[0;32m     42\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m     43\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39mbatch_input_ids,\n\u001b[0;32m     44\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mbatch_attention_masks,\n\u001b[0;32m     45\u001b[0m         labels\u001b[38;5;241m=\u001b[39mbatch_labels,\n\u001b[0;32m     46\u001b[0m         intensity_labels\u001b[38;5;241m=\u001b[39mbatch_intensity_labels\n\u001b[0;32m     47\u001b[0m     )\n\u001b[0;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 50\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     52\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n",
      "File \u001b[1;32mc:\\Users\\abc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "from torch.amp import autocast, GradScaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 10000\n",
    "# valid用 : 耐心指數\n",
    "best_val_loss = float('inf')\n",
    "best_avg_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "min_delta = 0.001  # 最小损失改善\n",
    "\n",
    "\n",
    "label_to_id = {'O': 0, 'B-Aspect': 1, 'I-Aspect': 2, 'B-Opinion': 3, 'I-Opinion': 4}\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "num_labels = len(label_to_id)\n",
    "config = BertConfig.from_pretrained('bert-base-chinese', num_labels=num_labels)\n",
    "model = ASTEModel.from_pretrained('bert-base-chinese', config=config)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "model.to(device)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # 训练循环中的数据加载\n",
    "    for step, batch in enumerate(tqdm(train_loader)):\n",
    "        batch_input_ids = batch['input_ids'].to(device)\n",
    "        batch_attention_masks = batch['attention_mask'].to(device)\n",
    "        batch_labels = batch['labels'].to(device)\n",
    "        batch_intensity_labels = batch['intensity_labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(\"cuda\"):\n",
    "            outputs = model(\n",
    "                input_ids=batch_input_ids,\n",
    "                attention_mask=batch_attention_masks,\n",
    "                labels=batch_labels,\n",
    "                intensity_labels=batch_intensity_labels\n",
    "            )\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss}\")\n",
    "\n",
    "    # **验证步骤**\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    all_pred_triplets = []\n",
    "    all_gold_triplets = []\n",
    "\n",
    "    for batch in tqdm(test_loader):\n",
    "        batch_input_ids = batch['input_ids'].to(device)\n",
    "        batch_attention_masks = batch['attention_mask'].to(device)\n",
    "        batch_labels = batch['labels'].to(device)\n",
    "        batch_intensity_labels = batch['intensity_labels'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=batch_input_ids,\n",
    "                attention_mask=batch_attention_masks,\n",
    "                labels=batch_labels,\n",
    "                intensity_labels=batch_intensity_labels\n",
    "            )\n",
    "        loss = outputs[0]\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = outputs[1]  # 模型的输出 logits\n",
    "        predictions = torch.argmax(logits, dim=2)  # 获取预测的标签\n",
    "\n",
    "        # 遍历批次中的每个样本\n",
    "        for i in range(batch_input_ids.size(0)):\n",
    "            input_ids = batch_input_ids[i]\n",
    "            pred_labels = predictions[i]\n",
    "            gold_labels = batch_labels[i]\n",
    "            attention_mask = batch_attention_masks[i]\n",
    "            intensity_labels = batch_intensity_labels[i]\n",
    "\n",
    "            # 将 input_ids 转换为 tokens\n",
    "            tokens = tokenizer.convert_ids_to_tokens(input_ids.cpu().numpy())\n",
    "\n",
    "            # 获取预测标签和真实标签\n",
    "            pred_tags = [id_to_label[label_id.item()] for label_id in pred_labels]\n",
    "            gold_tags = [id_to_label[label_id.item()] for label_id in gold_labels]\n",
    "\n",
    "            # 提取预测的三元组\n",
    "            pred_triplets = extract_triplets(tokens, pred_tags, model, input_ids, attention_mask)\n",
    "            # 提取真实的三元组\n",
    "            gold_triplets = extract_gold_triplets(tokens, gold_tags, intensity_labels)\n",
    "\n",
    "            all_pred_triplets.extend(pred_triplets)\n",
    "            all_gold_triplets.extend(gold_triplets)\n",
    "\n",
    "    # 计算评估指标\n",
    "    precision, recall, f1 = compute_metrics(all_pred_triplets, all_gold_triplets)\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(test_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss},Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "\n",
    "    # 检查验证损失是否改善\n",
    "    if ((avg_loss < best_avg_loss - min_delta) | (avg_val_loss < best_val_loss - min_delta)):\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        # 保存当前最好的模型\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"验证损失未改善，耐心值计数：{patience_counter}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"验证损失在连续多个 epoch 中未改善，停止训练。\")\n",
    "            break\n",
    "\n",
    "print(\"--------------\")\n",
    "print(\"训练完成\")\n",
    "# 保存模型\n",
    "output_dir = './saved_model/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    " 1. 使用自定义的 ASTEModel 进行预测\n",
    " 2. 提取实体并格式化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:33:38.989099Z",
     "iopub.status.busy": "2024-11-15T13:33:38.988417Z",
     "iopub.status.idle": "2024-11-15T13:33:39.270020Z",
     "shell.execute_reply": "2024-11-15T13:33:39.269057Z",
     "shell.execute_reply.started": "2024-11-15T13:33:38.989060Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', '整', '体', '上', '菜', '速', '度', '非', '常', '快', '。', '[SEP]']\n",
      "Predicted Labels: ['O', 'O', 'O', 'B-Aspect', 'I-Aspect', 'I-Aspect', 'I-Aspect', 'B-Opinion', 'I-Opinion', 'I-Opinion', 'O', 'O']\n",
      "Aspects: [{'text': '上菜速度', 'start': 2, 'end': 6, 'positions': [3, 4, 5, 6]}]\n",
      "Opinions: [{'text': '非常快', 'start': 6, 'end': 9, 'intensity': '3#3'}]\n",
      "ID\tTriplets\n",
      "R3530:S002\t(上菜速度, 非常快, 3#3)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertConfig, BertTokenizerFast\n",
    "\n",
    "# 定义标签映射\n",
    "label_to_id = {'O': 0, 'B-Aspect': 1, 'I-Aspect': 2, 'B-Opinion': 3, 'I-Opinion': 4}\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "\n",
    "# 定义设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载配置和自定义模型 ASTEModel\n",
    "config = BertConfig.from_pretrained('./saved_model/', num_labels=len(label_to_id))\n",
    "model = ASTEModel.from_pretrained('./saved_model/', config=config)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 加载 tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('./saved_model/')\n",
    "\n",
    "def predict(sentence):\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='pt',\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    offset_mapping = encoding['offset_mapping'][0]  # batch_size=1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[0]  # 获取预测的标签 logits\n",
    "    predictions = torch.argmax(logits, dim=2)  # [1, seq_len]\n",
    "    predictions = predictions[0].cpu().numpy()  # 转换为 numpy 数组\n",
    "\n",
    "    # 将 label id 转换为 label 名称\n",
    "    predicted_labels = [id_to_label[int(label_id)] for label_id in predictions]\n",
    "\n",
    "    # 将 tokens 转换回文字\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # 去除填充部分\n",
    "    effective_length = (attention_mask[0] == 1).sum().item()\n",
    "    tokens = tokens[:effective_length]\n",
    "    predicted_labels = predicted_labels[:effective_length]\n",
    "    offset_mapping = offset_mapping[:effective_length]\n",
    "\n",
    "    # 获取模型的序列输出，用于情感强度预测\n",
    "    with torch.no_grad():\n",
    "        bert_outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = bert_outputs.last_hidden_state[0, :effective_length, :]  # [seq_len, hidden_size]\n",
    "\n",
    "    return tokens, predicted_labels, offset_mapping, sequence_output\n",
    "\n",
    "def get_entities(tokens, labels, offsets, sequence_output):\n",
    "    aspects = []\n",
    "    opinions = []\n",
    "    idx = 0\n",
    "    while idx < len(labels):\n",
    "        label = str(labels[idx])  # 确保 label 是字符串\n",
    "        if label == 'B-Aspect':\n",
    "            aspect_tokens = [tokens[idx]]\n",
    "            aspect_start = offsets[idx][0].item()\n",
    "            aspect_end = offsets[idx][1].item()\n",
    "            aspect_positions = [idx]\n",
    "            idx += 1\n",
    "            while idx < len(labels) and str(labels[idx]) == 'I-Aspect':\n",
    "                aspect_tokens.append(tokens[idx])\n",
    "                aspect_end = offsets[idx][1].item()\n",
    "                aspect_positions.append(idx)\n",
    "                idx += 1\n",
    "            aspect_text = ''.join([token.lstrip('##') for token in aspect_tokens])\n",
    "            aspects.append({'text': aspect_text, 'start': aspect_start, 'end': aspect_end, 'positions': aspect_positions})\n",
    "        elif label == 'B-Opinion':\n",
    "            opinion_tokens = [tokens[idx]]\n",
    "            opinion_start = offsets[idx][0].item()\n",
    "            opinion_end = offsets[idx][1].item()\n",
    "            opinion_positions = [idx]\n",
    "            idx += 1\n",
    "            while idx < len(labels) and str(labels[idx]) == 'I-Opinion':\n",
    "                opinion_tokens.append(tokens[idx])\n",
    "                opinion_end = offsets[idx][1].item()\n",
    "                opinion_positions.append(idx)\n",
    "                idx += 1\n",
    "            opinion_text = ''.join([token.lstrip('##') for token in opinion_tokens])\n",
    "\n",
    "            # 获取观点词的隐藏状态\n",
    "            opinion_hidden_states = sequence_output[opinion_positions, :]  # [opinion_len, hidden_size]\n",
    "            # 计算平均表示\n",
    "            opinion_representation = opinion_hidden_states.mean(dim=0)  # [hidden_size]\n",
    "            # 预测情感强度\n",
    "            with torch.no_grad():\n",
    "                intensity_pred = model.regressor(opinion_representation.unsqueeze(0))  # [1, 2]\n",
    "                intensity_pred = intensity_pred.squeeze(0).detach().cpu().numpy()\n",
    "                # 四舍五入为整数\n",
    "                valence = int(round(intensity_pred[0]))\n",
    "                arousal = int(round(intensity_pred[1]))\n",
    "                intensity = f\"{valence}#{arousal}\"\n",
    "\n",
    "            opinions.append({\n",
    "                'text': opinion_text,\n",
    "                'start': opinion_start,\n",
    "                'end': opinion_end,\n",
    "                'intensity': intensity\n",
    "            })\n",
    "        else:\n",
    "            idx += 1\n",
    "    return aspects, opinions\n",
    "\n",
    "def format_output(sentence_id, aspects, opinions):\n",
    "    triplets = []\n",
    "    for aspect in aspects:\n",
    "        for opinion in opinions:\n",
    "            # 使用预测的情感强度\n",
    "            intensity = opinion['intensity']\n",
    "            triplet = f\"({aspect['text']}, {opinion['text']}, {intensity})\"\n",
    "            triplets.append(triplet)\n",
    "    triplets_str = ' '.join(triplets)\n",
    "    return f\"{sentence_id}\\t{triplets_str}\"\n",
    "\n",
    "# 示例句子\n",
    "sentence_id = 'R3530:S002'\n",
    "sentence = '整体上菜速度非常快。'\n",
    "\n",
    "# 预测\n",
    "tokens, predict_labels, offsets, sequence_output = predict(sentence)\n",
    "\n",
    "# 提取实体并预测情感强度\n",
    "aspects, opinions = get_entities(tokens, predict_labels, offsets, sequence_output)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Predicted Labels:\", predict_labels)\n",
    "print(\"Aspects:\", aspects)\n",
    "print(\"Opinions:\", opinions)\n",
    "\n",
    "# 格式化输出\n",
    "output_line = format_output(sentence_id, aspects, opinions)\n",
    "print('ID\\tTriplets')\n",
    "print(output_line)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6076231,
     "sourceId": 9893197,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
